{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_gVX_yHN5LlD",
    "outputId": "7e6e93a4-3028-41d7-d105-c149de85e90a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\vince\\appdata\\roaming\\python\\python39\\site-packages (3.8.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\vince\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: click in c:\\users\\vince\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\vince\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\vince\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\vince\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Vince\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\Vince\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#setup\n",
    "!pip install --user -U nltk\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.corpus import reuters\n",
    "from nltk.corpus import abc\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.lm import Vocabulary\n",
    "\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from math import log, fsum, exp\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zANoqkBXnurA"
   },
   "source": [
    "# **Language Model**\n",
    "***\n",
    "A Language Model (**LM**) is a model that given a sequence of words, or a sentence, is able to compute the probability of the sentence or sequence of words. Formally, an LM is a **probability distribution** over the sequence of words:\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "P(W) = P(w_1,w_2,w_3,...w_n)\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $W$ is the input sentence.\n",
    "- $n$ is the number of words in the sentence.\n",
    "- $w_i$ is the i-th word of the sentence.\n",
    "\n",
    "One of the related task to LM is to compute the probability that a word has of appearing given a sequence:\n",
    "\\begin{align*}\n",
    "P(w_n | w_1,w_2,w_3,...w_{n-1})\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_mapvy6XIpo"
   },
   "source": [
    "### CONSTRUCTION OF TRAINING TRAINING SET AND TEST SET\n",
    "we construct our corpus sentence, in which we have for each sentence a list of words.\n",
    "After that we split our list into two parts:\n",
    "\n",
    "\n",
    "1.   80% into **training set**\n",
    "2.   20% into **test set**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5Q4LGaoh82oC"
   },
   "outputs": [],
   "source": [
    "corpus_sentences_nltk = gutenberg.sents(gutenberg.fileids()[:100])\n",
    "corpus_sentences = []\n",
    "\n",
    "#For each sentence we remove the punctuations\n",
    "for sentence in corpus_sentences_nltk:\n",
    "  tmp = [] \n",
    "  for word in sentence:\n",
    "    #This regular expression is used for removing punctuation\n",
    "    word = re.sub(r'[^\\w\\s]', '', word)\n",
    "    if (word != ''):\n",
    "      tmp.append(word.lower())\n",
    "  if (len(tmp)!=0): \n",
    "    corpus_sentences.append(tmp) \n",
    "\n",
    "random.shuffle(corpus_sentences)\n",
    "#We split our corpus in training set (80%) and test set (20%)\n",
    "training_set = corpus_sentences[:int(len(corpus_sentences)*0.80)]\n",
    "test_set = corpus_sentences[int(len(corpus_sentences)*0.80):]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cvj-TaoVvnJc"
   },
   "source": [
    "## **How work Language Model**\n",
    "***\n",
    "The construction of an LM is based on **n-grams**\n",
    "### **N-Gram**\n",
    "An N-gram is a sequence of N words, for example given n = 2 and a corpus like : \n",
    "\n",
    "\"I study NLP\"\n",
    "\n",
    "the bigrams is: {I study, study NLP}.\n",
    "\n",
    "So, given a word $w_n$ and a sequence ranging from $w_1$ to $w_{n-1}$, we can compute the probability that $w_n$ has of appearing immediately after the sequence from $w_1$ to $w_{n-1}$, and this is the **N-gram probability**.\n",
    "\n",
    "\n",
    "Formally, in general, the **N-gram probability**, given a word $w_n$ and a sequence ranging from $w_1$ to $w_{n-1}$, is a **fraction** where at the denominator we have **the number of occurrences of the n-1-gram** and at the numerator **the number of occurrences of the n-gram**:\n",
    "\n",
    "\\begin{align*}\n",
    "P(w_n | w_1^{N-1}) = \\frac{C(w_1^{N-1}w_{N})}{C(w_1^{N-1})}\n",
    "\\end{align*}\n",
    "\n",
    "**with**:\n",
    "\\begin{align*}\n",
    "C(w_1^{N-1}w_N) = C(w_1^N)\n",
    "\\end{align*}\n",
    "\n",
    "**where**\n",
    "- $C(w_1^{N-1}w_{N})$ is the number of occurrences of the n-gram.\n",
    "- $C(w_1^{N-1})$ is the number of occurrences of the n-1-gram.\n",
    "\n",
    "### **Sequence probabilities**\n",
    "In order to compute the probability of an entire sequence, is necessary to apply the **Chain Rule**, which is the **joint N-gram probability** of each word of the sequence:\n",
    "\n",
    "\\begin{align*}\n",
    "P(w_1w_2...w_n) = \\prod_{i=1}^{N}P(w_1|w_1w_2...w_{i-1})\n",
    "\\end{align*}\n",
    "\n",
    "Since for very large *N* there is the risk that the probability is 0, thus canceling the whole product, it is used the **approximation of sequence probability** where we fixed tha value of *N*. \n",
    "\n",
    "For example, if we fixed *N=2* in the calculation of the joint probability don't consider the entire previous sequence but only the previous word:\n",
    "\\begin{align*}\n",
    "P(w_1w_2...w_n) â‰ˆ \\prod_{i=1}^{N}P(w_i|w_{i-1})\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "In order to compute the probability of the sequence it is necessary:\n",
    "- Add **n-1 start of sentence token as $<s>$** at the beginning of the sentence, this is done in order to compute the n-gram probability of the initial word.\n",
    "- Add 1 **end of sentence token as $</s>$** for two reasons:\n",
    "  - differentiate the same word when it appears at the end of the sentence versus when it appears anywhere else.\n",
    "  - make sure that the sum of the probabilities of all sentences of any length is 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJfPg6nft8Yk"
   },
   "source": [
    "### **UNKNOWN WORDS**\n",
    "When using Language Model in real tasks, it happens to deal with words that did not occurr in the training set. We need to handle word missing from a training corpus.\n",
    "We need to:\n",
    "\n",
    "\n",
    "*   Update training corpus with *\\<UNK>*\n",
    "*   Construct a vocabulary\n",
    "\n",
    "A way to train the LM for unknow words is to create a vocabulary implicitly, and replace in the training corpus words with frequency is less than 2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6UC4ND61u3CQ"
   },
   "source": [
    "In this code we construct a dictionary in which we saved for each word the frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ecIm4v0KQLDF"
   },
   "outputs": [],
   "source": [
    "dict_word_freq = {}\n",
    "#For each word we compute the frequencies in all the training set\n",
    "for sentence in training_set:\n",
    "  for word in sentence:\n",
    "    dict_word_freq[word] = dict_word_freq.get(word,0) + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tX-ridWiXRbn"
   },
   "source": [
    "We replace words in training corpus with frequency < 10 with \\<UNK>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Mt4kPbhUQ6DV"
   },
   "outputs": [],
   "source": [
    "#For each sentence in training set we replace words with freq < 10 with <UNK>\n",
    "for sentence in training_set:\n",
    "  for index_word,word in enumerate(sentence):\n",
    "    if (dict_word_freq[word]<10):\n",
    "      sentence[index_word] = \"<UNK>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2GdNndhvRfl"
   },
   "source": [
    "We define a function where give in input the training corpus return the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "rr2o9Z1bWflp"
   },
   "outputs": [],
   "source": [
    "#We define a funcion where we create the vocabulary\n",
    "def get_vocabulary(training_set):\n",
    "  vocabulary_list = []\n",
    "  for sentence in training_set:\n",
    "    for word in sentence:\n",
    "      if(word not in vocabulary_list and word != \"<UNK>\" and word!= \"<e>\"):\n",
    "        vocabulary_list.append(word)\n",
    "  return vocabulary_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mqV8UM5yvZUY"
   },
   "source": [
    "### **CONSTRUCTION OF N-GRAM LANGUAGE MODEL**\n",
    "For the construction of N-gram Language Model we need:\n",
    "\n",
    "\n",
    "*   Count Matrix\n",
    "*   Probability Matrix with log probability to avoid underflow\n",
    "\n",
    "Recall that we are considering N-grams approximation, where we fixed the N and we compute for each N-gram in our training corpus the conditional probabilities, for avoiding problem of 0 we add-k smoothing:\n",
    "\\begin{align*}\n",
    "P(w_n | w_{n-N+1}^{N-1}) = \\frac{C(w_{n-N+1}^{N-1},w_{N})+k}{C(w_{n-N+1}^{N-1})+|V|*k}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xCDATmnwwy23"
   },
   "source": [
    "### **COUNT MATRIX**\n",
    "In this section of code we define a function where in input we have:\n",
    "\n",
    "\n",
    "*   *N* of N-gram\n",
    "*   *training corpus* \n",
    "\n",
    "And return two dictionary where for each N-gram we store the numerator and denominator:\n",
    "\n",
    "\n",
    "1.   the first ***count_dictionary*** contains the entries of the count matrix, in which we store the frequencies of each N-gram in our training set\n",
    "2.   the second ***rowsum*** is used for row sum, where we store the sum of the frequencies of each (N-1)-gram of the N-grams in our training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "YFhxP8ngXZYf"
   },
   "outputs": [],
   "source": [
    "#In this function we create the count matrix, for avoiding the problem where most entries are 0 we use a dictionary.\n",
    "\n",
    "'''\n",
    "  We define two dictionary:\n",
    "    - the first \"count_dictionary\" contains the entries of the count matrix, in which we store the frequencies of each N-gram in our training set\n",
    "    - the second \"rowsum\" is used for row sum, where we store the sum of the frequencies of each (N-1) previous words of the N-grams in our training set\n",
    "'''\n",
    "def get_count_matrix(n,training_set):\n",
    "  count_dictionary = {}\n",
    "  rowsum = {}\n",
    "  ngrams_corpus = []\n",
    "  for sentence in training_set:\n",
    "    #For each sentence we add <e> in the final position, this because the function ngram of nltk add N ending symbol, but we work only with one\n",
    "    if(sentence[len(sentence)-1] != \"<e>\"):\n",
    "      sentence.append(\"<e>\")\n",
    "    ngrams_corpus = (list(ngrams(sentence, n, pad_left=True, left_pad_symbol='<s>')))\n",
    "    for ngram in ngrams_corpus:\n",
    "      key_row = ngram[0:n-1]\n",
    "      count_dictionary[ngram]= count_dictionary.get(ngram,0) + 1\n",
    "      rowsum[key_row] = rowsum.get(key_row,0) + 1\n",
    "  return count_dictionary,rowsum\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "INVx7aV51L57"
   },
   "source": [
    "### **PROBABILITY MATRIX**\n",
    "Now we define a function where the input is formed by:\n",
    "\n",
    "\n",
    "*   ***count_dictionary*** that represents the numerator without the k-smoothing\n",
    "*   ***rowsum*** that represents the denominator without the k-smoothing\n",
    "*   ***n*** of N-gram\n",
    "*   ***V*** is the size of the vocabulary\n",
    "\n",
    "In this function we compute the ratio of the previous formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "z3QUqHm1VsAH"
   },
   "outputs": [],
   "source": [
    "#In this function we create the probability matrix\n",
    "def get_probability_matrix(count_dictionary,rowsum,n, V):\n",
    "  probability_dictionary = {}\n",
    "  k_smoothing = 0.05\n",
    "  #For each value of each N-gram we divide it with the corresponding row sum where the row corresponds to the (N-1)-gram of the N-gram.\n",
    "  for key,value in count_dictionary.items():\n",
    "    key_row = key[0:n-1] #In this key_row we store the key for the dictionary rowsum, that is equal to the (N-1)-gram of the N-gram\n",
    "    probability_dictionary[key] = log((count_dictionary[key] + k_smoothing) / (rowsum.get(key_row,0) + (V * k_smoothing)),2)\n",
    "  return probability_dictionary\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KSgrBB2Y8tBb"
   },
   "source": [
    "From a training corpus we can construct:\n",
    "\n",
    "\n",
    "*   Vocabulary\n",
    "*   Count matrix in the code is ***count_dictionary*** and ***rowsum***\n",
    "*   Probability matrix in the code is represented by ***probability_dictionary***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ulV4in1kfNUz",
    "outputId": "a816aa3e-8cae-4582-ad48-4ae98f89baa8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[COUNT = ('thyself', 'accompanied', '<UNK>') 1 ] [ROWSUM = ('thyself', 'accompanied') 1 ] [PROBABILITY = 0.002370470707754824 ]\n",
      "[COUNT = ('accompanied', '<UNK>', 'not') 1 ] [ROWSUM = ('accompanied', '<UNK>') 1 ] [PROBABILITY = 0.002370470707754824 ]\n",
      "[COUNT = ('<UNK>', 'not', 'social') 1 ] [ROWSUM = ('<UNK>', 'not') 312 ] [PROBABILITY = 0.001392665296107169 ]\n",
      "[COUNT = ('not', 'social', 'communication') 1 ] [ROWSUM = ('not', 'social') 1 ] [PROBABILITY = 0.002370470707754824 ]\n",
      "[COUNT = ('social', 'communication', 'yet') 1 ] [ROWSUM = ('social', 'communication') 1 ] [PROBABILITY = 0.002370470707754824 ]\n",
      "[COUNT = ('communication', 'yet', 'so') 1 ] [ROWSUM = ('communication', 'yet') 1 ] [PROBABILITY = 0.002370470707754824 ]\n",
      "[COUNT = ('yet', 'so', 'pleased') 1 ] [ROWSUM = ('yet', 'so') 17 ] [PROBABILITY = 0.0022878309184006964 ]\n",
      "[COUNT = ('so', 'pleased', 'canst') 1 ] [ROWSUM = ('so', 'pleased') 9 ] [PROBABILITY = 0.0023284177846767933 ]\n",
      "[COUNT = ('pleased', 'canst', 'raise') 1 ] [ROWSUM = ('pleased', 'canst') 1 ] [PROBABILITY = 0.002370470707754824 ]\n",
      "[COUNT = ('canst', 'raise', 'thy') 1 ] [ROWSUM = ('canst', 'raise') 1 ] [PROBABILITY = 0.002370470707754824 ]\n"
     ]
    }
   ],
   "source": [
    "N = 3 #We define N of the N-gram approximation\n",
    "vocabulary_list = get_vocabulary(training_set)\n",
    "\n",
    "count_dictionary, rowsum= get_count_matrix(N,training_set)\n",
    "\n",
    "probability_dictionary = get_probability_matrix(count_dictionary, rowsum,N, len(vocabulary_list))\n",
    "\n",
    "#ONLY FOR THE PRINT\n",
    "for key, value in list(probability_dictionary.items())[10:20]:\n",
    "  print(\"[COUNT =\", key, count_dictionary[key],\"] [ROWSUM =\",key[0:N-1], rowsum[key[0:N-1]], \"] [PROBABILITY =\", pow(2,value),\"]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dq7PyH669iBZ"
   },
   "source": [
    "### **BUILDING TEST SET**\n",
    "In this function we process our test set in which for each sentece we replace the words that are not in vocabulary with the tag \\<UNK> and add the ending symbol *\\<e>* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "W0KwUiGLps1n"
   },
   "outputs": [],
   "source": [
    "#In this function we process the test set:\n",
    "def process_test_set(test_set, vocabulary_list):\n",
    "  for sentence in test_set:\n",
    "    for index, word in enumerate(sentence):\n",
    "      #We replace the words that are no present in the dictionary with <UNK>\n",
    "      if (word not in vocabulary_list and word != \"<e>\"):\n",
    "        sentence[index] = \"<UNK>\"\n",
    "    #We add the ending symbol \"<e>\", this help use for the computation of perplexity!\n",
    "    if(sentence[len(sentence)-1] != \"<e>\"):\n",
    "      sentence.append(\"<e>\")\n",
    "  return test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmqkTviE-HLK"
   },
   "source": [
    "### **PERPLEXITY - A MEASURE OF QUALITY OF LM**\n",
    "Given a sentence:\n",
    "\\begin{align*}\n",
    "W = w_1, w_2, ... , w_m\n",
    "\\end{align*}\n",
    "and its probability:\n",
    "\\begin{align*}\n",
    "P(W) = P(w_1, w_2, ... , w_m)\n",
    "\\end{align*}\n",
    "If it is higher, the more accurate (or even realistic) the sentence ***W*** is.\n",
    "\n",
    "Given a laguage mode, we do not use directly the probability of a test sentence to evaluate the LM effectiveness, rather we use a measure called **PERPLEXITY** defined as:\n",
    "\n",
    "\\begin{align*}\n",
    "PP(W) = P(w_1, w_2, ... , w_m)^{-1/m} = \\sqrt[m]{\\frac{1}{P(w_1,w_2, ..., w_m)}} = \\sqrt[N]{\\prod_{i=1}^{m}\\frac{1}{P(w_i|w_1, w_2, ..., w_{i-1})}}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "if we use a bigram LM, the the perplexity become:\n",
    "\\begin{align*}\n",
    "\\sqrt[m]{\\prod_{i=1}^{m}\\frac{1}{P(w_i|w_{i-1})}}\n",
    "\\end{align*}\n",
    "\n",
    "The LM can be evaluated on test sets using the **PERPLEXITY METRIC**\n",
    "\\begin{align*}\n",
    "PP(W) = P(s_1, s_2, ... , s_N)^{-1/m}\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "\n",
    "\n",
    "*   ***W*** -> test set containing ***N*** sentences ${s_j}$, $j=1, ..., N$\n",
    "*   ${s_j}$ -> j-th sentence in the test set, each ending with ***\\</s>***\n",
    "*   ***m*** -> number of all words in the entire test set ***W*** including ***\\</s>*** but not including ***\\<s>***.\n",
    "\n",
    "For erase of computation, the log peplexity is often used:\n",
    "\\begin{align*}\n",
    "\\log_2 PP(W) = - \\frac{1}{m} * \\sum_{i=1}^{m}\\log_2 (P(w_i|w_{i-1}))\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "PPL = 2^{\\log_2 PP(W)}\n",
    "\\end{align*}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lwfwYeytiD8N",
    "outputId": "c255e1d3-7fb4-4ae3-f3e5-9e14d24dfe75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERPLEXITY:  954.2482469286284\n"
     ]
    }
   ],
   "source": [
    "ngrams_ = []\n",
    "probability = []\n",
    "total_words = 0\n",
    "k_smoothing = 0.05\n",
    "test_set_processed = process_test_set(test_set, vocabulary_list)\n",
    "for sentence in test_set_processed:\n",
    "    _sum = 0\n",
    "    total_words = total_words + len(sentence) #We sum the length of each sentence for the computation of perplexity\n",
    "    ngrams_ =  (list(ngrams(sentence, N, pad_left=True, left_pad_symbol='<s>'))) #We compute the N-grams for each sentence of the test set\n",
    "    for ngram in ngrams_:\n",
    "      #We check if N-gram is contained in probability_dictionary we pick that value\n",
    "      if (ngram in probability_dictionary): \n",
    "        _sum = _sum + probability_dictionary[ngram]\n",
    "      else:\n",
    "      #Otherwise we estimate the probability using the k-smoothing\n",
    "        key_row = ngram[0:N-1]\n",
    "        tmp = (count_dictionary.get(ngram,0) + k_smoothing) / (rowsum.get(key_row,0) + (len(vocabulary_list) * k_smoothing))\n",
    "        _sum = _sum + log(tmp,2)\n",
    "    probability.append(_sum)\n",
    " \n",
    "#We compute the log_perplexity\n",
    "log_perplexity = (-1/total_words) * (fsum(probability))\n",
    "\n",
    "#We do the inverse of logarithm in order to have the effective value of perplexity\n",
    "perplexity = pow(2,log_perplexity)\n",
    "print(\"PERPLEXITY: \",perplexity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njKea-ESGDk6"
   },
   "source": [
    "In this function we compute upcoming word given the previus N-1 gram, and return a dictionary where for each word we have the probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "T0q4h19OVN8W"
   },
   "outputs": [],
   "source": [
    "#We define a function that suggest the upcoming words with their probability\n",
    "def get_upcoming_words(last_ngram, probability_dictionary):\n",
    "  probability_list = {}\n",
    "\n",
    "#We consider all tha N-gram that contain the (N-1)-gram. (N-1)-gram is the last in our phrase\n",
    "  for key,value in probability_dictionary.items():\n",
    "    key_row = (key[0:N-1])\n",
    "    if(key_row == last_ngram):\n",
    "      probability_list[key[N-1]] = value\n",
    "  probability_list.pop(\"<UNK>\",None)\n",
    "  sorted_prob= dict(sorted(probability_list.items(), key=lambda x:x[1],reverse=True))\n",
    "  return sorted_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZjOqCegGCnk"
   },
   "source": [
    "Given a phrase we compute the upcoming words and print the 5 most probable words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Y_zPVSM7nZ_",
    "outputId": "2414bbdf-0d19-4dc8-ebe6-4d550235d800"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i have just been  -> Probability = 0.008729388942773999\n",
      "i have just such  -> Probability = 0.002263174911089555\n",
      "i have just taken  -> Probability = 0.002263174911089555\n",
      "i have just now  -> Probability = 0.002263174911089555\n",
      "i have just freed  -> Probability = 0.002263174911089555\n",
      "i have just felt  -> Probability = 0.002263174911089555\n",
      "i have just ask  -> Probability = 0.002263174911089555\n",
      "i have just prevailed  -> Probability = 0.002263174911089555\n",
      "i have just come  -> Probability = 0.002263174911089555\n",
      "i have just balances  -> Probability = 0.002263174911089555\n"
     ]
    }
   ],
   "source": [
    "#Our phrase\n",
    "phrase = \"i have just\"\n",
    "\n",
    "#We tokenize our phrase\n",
    "phrase_tokenize = [word.lower() for word in word_tokenize(phrase)]\n",
    "\n",
    "#We consider only the lase N-1 gram\n",
    "last_ngram = tuple(phrase_tokenize[len(phrase_tokenize)-N+1:len(phrase_tokenize)])\n",
    "\n",
    "#Compute the upcoming word\n",
    "upcoming_words = get_upcoming_words(last_ngram,probability_dictionary)\n",
    "for key, value in list(upcoming_words.items())[:10]:\n",
    "  print(phrase, key, \" -> Probability =\", pow(2,value))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
